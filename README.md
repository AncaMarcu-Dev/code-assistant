# ğŸ§  Local Code Assistant (TypeScript + Ollama + ChromaDB)

A personal AI assistant that answers questions about your code using local LLMs, local embeddings, and real vector search â€” no paid APIs required.

---

## âš™ï¸ Features

- âœ… Written in TypeScript
- ğŸ§  Uses **Ollama** + `nomic-embed-text` for local embeddings
- ğŸ” Uses **ChromaDB** for vector search
- ğŸ¤– Designed to run fully offline
- ğŸ§ª Educational and modular for learning AI dev skills

---

## ğŸ§© Requirements

- [Node.js](https://nodejs.org/) (v18+)
- [Ollama](https://ollama.com/) (running locally)
- [Python](https://www.python.org/) + `pip` (for ChromaDB)
- Git / Terminal

---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the project

```bash
git clone https://github.com/your-name/code-assistant.git
cd code-assistant
```

---

### 2. Install dependencies

```bash
npm install
```

---

### 3. Start Ollama and pull embedding model

```bash
ollama serve
ollama pull nomic-embed-text
```

---

### 4. Set up and start ChromaDB

```bash
pip install chromadb
chromadb run
```

This starts a local vector DB at `http://localhost:8000`.

---

### 5. Embed code snippets

Edit your `scripts/embed.ts` to include your own code chunks, then run:

```bash
ts-node scripts/embed.ts
```

This will:
- Embed code using `nomic-embed-text`
- Upload to ChromaDB for later search

---

### 6. Ask a question

Ask a question about your code:

```bash
npm run start -- "What does handleLogin do?"
```

Or run interactively:

```bash
npm run start
# Then type your question
```

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ getRelevantChunks.ts
â”‚   â”‚   â”œâ”€â”€ llm.ts
â”‚   â”‚   â””â”€â”€ qa.ts
â”‚   â””â”€â”€ index.ts
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ embed.ts
â”œâ”€â”€ .env (optional)
â”œâ”€â”€ tsconfig.json
â””â”€â”€ package.json
```

---

## ğŸ§  How It Works

1. Your code is split into chunks (manually or automated)
2. Each chunk is embedded and stored in ChromaDB
3. A question is embedded using the same model
4. ChromaDB returns the most relevant code snippets
5. Those snippets + the question are sent to an LLM for a final answer

---

## ğŸ¤” FAQ

**Q: Does this use OpenAI or any paid service?**  
**A:** Nope! Everything runs locally using free/open-source tools.

**Q: Can I use a different embedding model or LLM?**  
**A:** Yes! Ollama supports multiple models like `mistral`, `llama3`, `codellama`, etc.

**Q: How do I add more code?**  
**A:** Add more chunks to `scripts/embed.ts` or automate chunking later with a tool like LangChain.

---

## ğŸ“Œ Next Steps

- âœ… Add automatic code chunking
- âœ… Use local LLMs like `codellama` for answering
- ğŸ”œ Add context window management
- ğŸ”œ Build a UI with Streamlit or a CLI wizard

---

## ğŸ§‘â€ğŸ’» License

MIT â€” use freely and hack away!


# ğŸ§  Code Insight Assistant (Local, Offline, TypeScript + Python)

A personal AI assistant that answers questions about your code using local LLMs, local embeddings, and real vector search. **Fully offline. No paid APIs.**

---

## âš™ï¸ Features

* âœ… Written in **TypeScript** (main logic)
* ğŸ§  Uses **Ollama** for local LLM (e.g., LLaMA3)
* ğŸ“ Uses **Python** server (`nomic-embed-text`) for local code embeddings
* ğŸ” In-memory vector search (cosine similarity)
* âš¡ Single `npm run start` command to launch everything
* ğŸ§© Easily extendable for CLI or UI-based interactions

---

## ğŸ“ Project Structure

```
project-root/
â”‚
â”œâ”€â”€ python/
â”‚   â””â”€â”€ embed-server.py       # Flask + nomic embedding API
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts              # App entrypoint
â”‚   â”œâ”€â”€ explain.ts            # Prompts LLM to explain code
â”‚   â”œâ”€â”€ embed.ts              # Calls Python embed server
â”‚   â”œâ”€â”€ search.ts             # In-memory vector search
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ pythonServer.ts   # Launches the Python server
â”‚
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### 1. ğŸ§° Install dependencies

#### TypeScript side

```bash
npm install
```

#### Python side

```bash
cd python
pip install -r requirements.txt
```

Contents of `python/requirements.txt`:

```txt
flask
nomic
```

---

### 2. ğŸ§  Start Ollama

Ensure Ollama is installed and running locally:

```bash
ollama run llama3
```

---

### 3. â–¶ï¸ Run the app

```bash
npm run start
```

This will:

* Automatically launch the Python embedding server
* Wait for it to initialize
* Accept your code snippet
* Embed it, store it, and query the LLM for explanation & suggestions

---

## ğŸ§ª Example Usage

Paste a code snippet when prompted:

```ts
function sum(a: number, b: number): number {
  return a + b;
}
```

And the assistant might respond:

> This is a simple TypeScript function that returns the sum of two numbers. Consider adding input validation.

---

## ğŸ§± How It Works

### ğŸ”  Embedding

* `embed-server.py` runs a Flask server and uses `nomic-embed-text`
* TypeScript app sends code to `http://localhost:8001/embed`

### ğŸ” Vector Search

* Simple in-memory cosine similarity
* Embeds are compared against past code chunks

### ğŸ§  LLM

* Ollama (e.g., LLaMA3) receives prompts like:

```text
Explain this code:
<code here>

Suggest improvements.
```

---

## ğŸ¤ License & Credits

* Built by you, powered by open-source models.
* Uses Ollama, nomic, Flask, TypeScript.

---

## â“ Want More?

* Add a web UI (React/Next.js)
* Persist embeddings to disk or database
* Use local file watcher to auto-index files

---

Enjoy your local, private, no-API-code assistant! ğŸš€

